ðŸ’¡ Overview:
Conducted an in-depth comparative analysis of ten popular deep learning optimizers (SGD, Adam, RMSProp, AdaGrad, AMSGrad, and more) on image classification tasks using ResNet50 and AlexNet architectures across CIFAR-10 and MNIST datasets.

ðŸ§  My Contribution:

Focused on implementing and evaluating all optimizers for ResNet50, including SGD variants and adaptive optimizers such as Adamax and AMSGrad.

Conducted extensive training experiments across multiple learning rates, ensuring controlled testing conditions with fixed hyperparameters.

Analyzed performance metrics such as training loss, test accuracy, and convergence speed to benchmark optimizer efficiency.

ðŸš€ Key Outcomes:

Achieved 99.7% test accuracy on MNIST and 90% on CIFAR-10 using ResNet50 with the proposed NASG (Nesterov Accelerated Shuffling Gradient) optimizer.

Demonstrated that NASG provides robust and consistent convergence across different learning rates and model complexities.

Provided practical insights into optimizer selection based on architecture and dataset characteristics.

ðŸ“ˆ Skills & Tools:
Python, PyTorch, Deep Learning, ResNet50, AlexNet, CIFAR-10, MNIST, Gradient Descent, Adaptive Optimizers, Optimization Algorithms, Training Convergence Analysis
